# Ultralytics YOLO ðŸš€, AGPL-3.0 license
# LoRA Training Configuration for YOLOV3U
# Usage: yolo train cfg=examples/lora_examples/yolov3_lora.yaml

# Global settings ------------------------------------------------------------------------------------------------------
task: detect # (str) YOLO task, i.e. detect, segment, classify, pose, obb
mode: train # (str) YOLO mode, i.e. train, val, predict, export, track, benchmark
device: mps # (int | str | list) device: 0 or [0,1,2,3] for CUDA, 'cpu'/'mps', or -1/[-1,-1] to auto-select idle GPUs

# Train settings -------------------------------------------------------------------------------------------------------
model: yolov3u.pt # (str, optional) path to model file, i.e. yolov8n.pt or yolov8n.yaml
data: coco128.yaml # (str, optional) path to data file, i.e. coco8.yaml
epochs: 50 # (int) number of epochs to train for
time: # (float, optional) max hours to train; overrides epochs if set
patience: 100 # (int) early stop after N epochs without val improvement
batch: 16 # (int) batch size; use -1 for AutoBatch
imgsz: 640 # (int | list) train/val use int (square); predict/export may use [h,w]
save: True # (bool) save train checkpoints and predict results
save_period: -1 # (int) save checkpoint every N epochs; disabled if < 1
cache: False # (bool | str) cache images in RAM (True/'ram') or on 'disk' to speed dataloading; False disables
workers: 8 # (int) dataloader workers (per RANK if DDP)
project: runs/lora_examples # (str, optional) project name for results root
name: yolov3_lora # (str, optional) experiment name; results in 'project/name'
exist_ok: False # (bool) overwrite existing 'project/name' if True
pretrained: True # (bool | str) use pretrained weights (bool) or load weights from path (str)
optimizer: auto # (str) optimizer: SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, or auto
verbose: True # (bool) print verbose logs during training/val
seed: 0 # (int) random seed for reproducibility
deterministic: True # (bool) enable deterministic ops; reproducible but may be slower
single_cls: False # (bool) treat all classes as a single class
rect: False # (bool) rectangular batches for train; rectangular batching for val when mode='val'
cos_lr: False # (bool) cosine learning rate scheduler
close_mosaic: 10 # (int) disable mosaic augmentation for final N epochs (0 to keep enabled)
resume: False # (bool) resume training from last checkpoint in the run dir
amp: True # (bool) Automatic Mixed Precision (AMP) training; True runs AMP capability check
fraction: 1.0 # (float) fraction of training dataset to use (1.0 = all)
profile: False # (bool) profile ONNX/TensorRT speeds during training for loggers
freeze: # (int | list, optional) freeze first N layers (int) or specific layer indices (list)
multi_scale: False # (bool) multiscale training by varying image size
compile: False # (bool | str) enable torch.compile() backend='inductor'; True="default", False=off, or "default|reduce-overhead|max-autotune-no-cudagraphs"

# Val/Test settings ----------------------------------------------------------------------------------------------------
val: True # (bool) run validation/testing during training
split: val # (str) dataset split to evaluate: 'val', 'test' or 'train'
save_json: False # (bool) save results to COCO JSON for external evaluation
conf: # (float, optional) confidence threshold; defaults: predict=0.25, val=0.001
iou: 0.7 # (float) IoU threshold used for NMS
max_det: 300 # (int) maximum number of detections per image
half: False # (bool) use half precision (FP16) if supported
dnn: False # (bool) use OpenCV DNN for ONNX inference
plots: True # (bool) save plots and images during train/val

# LoRA settings --------------------------------------------------------------------------------------------------------
lora_r: 16 # (int) LoRA rank (0 to disable)
lora_alpha: 32 # (int) LoRA alpha
lora_dropout: 0.05 # (float) LoRA dropout
lora_bias: "none" # (str) LoRA bias type: "none", "all", "lora_only"
lora_target_modules: ["conv"] # (list[str], optional) LoRA target modules
lora_save_adapters: True # (bool) Save LoRA adapters
lora_adapter_dir: "lora_adapter" # (str) Directory name for adapters
lora_auto_r_ratio: 0.0 # (float) Auto calculate LoRA rank based on params ratio
lora_use_dora: False # (bool) Enable DoRA (Weight-Decomposed Low-Rank Adaptation)
lora_type: "lora" # (str) PEFT type: "lora", "loha", "lokr"
lora_quantization: "none" # (str) Quantization type: "none", "4bit", "8bit" (Requires bitsandbytes)
lora_include_moe: False # (bool) Include MoE layers in auto-detection
lora_include_attention: False # (bool) Include Attention layers in auto-detection
lora_only_backbone: False # (bool) Only apply LoRA to backbone
lora_exclude_modules: # (list[str], optional) Modules to exclude
lora_last_n: # (int, optional) Only apply to last N layers
lora_from_layer: # (int, optional) Start applying from layer index
lora_to_layer: # (int, optional) Stop applying at layer index
lora_allow_depthwise: False # (bool) Allow depthwise convolution
lora_kernels: # (list[int], optional) Filter by kernel size
lora_gradient_checkpointing: True # (bool) Enable gradient checkpointing for LoRA memory optimization
